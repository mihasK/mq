[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/compare_df_pandas/index.html",
    "href": "posts/compare_df_pandas/index.html",
    "title": "Compare pandas ignoring NAN",
    "section": "",
    "text": "Problem\nIf you ever needed to check that dataframes are exactly equal in values, you might know it should not be done with regular == operator. The problem is in numpy, np.nan == np.nan gives False, and when you run something like\n\nimport pandas as pd\nimport numpy as np\n\n(\n    pd.Series([1,2,3, np.nan, 5]) == \n    pd.Series([1,2,3, np.nan, 5])\n).all()\n\nnp.False_\n\n\nit compares values elementwise and eventually gives False.\nTo try manually overcome this, with help of something like .fillna(..) - bad idea. You need different substitutors depending on dtype of the column, and with pd.Categorical dtype it’s especially troublesome - you need to extend your list of categories first.\nRight way to do this is using of pd.equals(...) (don’t confuse with pd.eq(...)!) or assert_…_equal in older versions of pandas:\n\nprint(\n    pd.Series([1,2,3, np.nan,5]).equals(\n        pd.Series([1,2,3, np.nan, 5])\n    )\n)\n\npd.testing.assert_series_equal(\n    pd.Series([1,2,3, np.nan,5]),\n    pd.Series([1,2,3, np.nan, 5])\n)\n\nTrue\n\n\nKeep in mind - usually you need to ignore columns order (it could have been reordered for some reason):\n\ndf1 = pd.DataFrame({\n    'a': [1,2,np.nan],\n    'b': ['x','y','z']\n})\ndf2 = pd.DataFrame({\n    'b': ['x','y','z'],\n    'a': [1,2,np.nan],\n})\nprint(\n    pd.DataFrame.equals(\n        df1,\n        df2\n    )\n)\n# Output: False\n\nprint(\n    pd.DataFrame.equals(\n        df1.sort_index(axis=1), \n        df2.sort_index(axis=1)\n    )\n)\n# Output: True\n\nFalse\nTrue\n\n\n\n\nUtility function\nSo, here’s a final utility function which hopefully will save you some time:\n\nimport pandas as pd\n\ndef __sort_indices(df, ignore_rows_order=True):\n    # Columns sorting\n    df = df.sort_index(axis=1)\n\n    # Rows index sorting\n    if ignore_rows_order:\n        df = df.sort_index()\n\n    return df\n\ndef is_df_eq(df1, df2, ignore_rows_order=True) -&gt; bool:\n    df1 = __sort_indices(df1, ignore_rows_order=ignore_rows_order)\n    df2 = __sort_indices(df2, ignore_rows_order=ignore_rows_order)\n\n    return pd.DataFrame.equals(df1, df2)\n\n\ndef print_equals_by_columns(df1: pd.DataFrame, df2, ignore_rows_order=True):\n    \"\"\"\n    Helps to identify which columns are not equal\n    \"\"\"\n\n    df1 = __sort_indices(df1, ignore_rows_order=ignore_rows_order)\n    df2 = __sort_indices(df2, ignore_rows_order=ignore_rows_order)\n        \n    if columns_diff := df1.columns.symmetric_difference(df2.columns).tolist():\n        print(\"Columns set is different:\", columns_diff)\n\n    print(\"Common columns equality:\")\n    for c in df1.columns.intersection(df2.columns):\n        print(c, ':', pd.Series.equals(\n            df1[c],\n            df2[c],\n        ))\n\n\n\nTests\nA bit of tests demonstrating the behaviour:\n\n\nTests\nimport numpy as np\n\ndf1 = pd.DataFrame({\n    'ID': [1,2,3],\n    'A': [1,2,3],\n    'B': [1,2,np.nan],\n    'C': ['x', 'y', 'Z']\n}).set_index('ID')\n\ndf2 = pd.DataFrame({\n    'ID': [1,2,3],\n    'A': [1,2,3],\n    'B': [1,2,np.nan],\n    'C': ['x', 'y', 'NOT_Z'],\n    'D': [0]*3\n}).set_index('ID')\n\ncommon_columns = ['A', 'B']\nprint(\n    f\"Is {common_columns} equal?:\", \n    is_df_eq(df1[common_columns], df2[common_columns])\n)\n\nprint(\n    \"Is equal?:\", is_df_eq(df1, df2)\n)\nprint_equals_by_columns(df1, df2)\n\n\n\nIs ['A', 'B'] equal?: True\nIs equal?: False\nColumns set is different: ['D']\nCommon columns equality:\nA : True\nB : True\nC : False"
  },
  {
    "objectID": "posts/just-make/just-make.html",
    "href": "posts/just-make/just-make.html",
    "title": "Just Make",
    "section": "",
    "text": "Usage of Makefiles have gone beyond original purpose of compilation and building of systems in C language.\nMany people use them for running arbitrary commands like setup, linting, formatting, etc., in projects built on different languages and stacks, inlcuding Python. For that purpose Makefile works fine, but has quite a lot of quirks related to its original purpose. Recently a better substitute for Makefile for broader use cases appeared - it is called Just.\nI’ve found that tool is very useful and decided to switch to it in all my projects."
  },
  {
    "objectID": "posts/just-make/just-make.html#makefile",
    "href": "posts/just-make/just-make.html#makefile",
    "title": "Just Make",
    "section": "Makefile",
    "text": "Makefile\nBut first let’s recall how a Makefile is typically used. This is an example of a Makefile I’m using for one of my projects:\n\n.PHONY: *\n\n\nactivate:\n    poetry shell\n\nlint:\n    poetry run ruff .\n\nwatchmedo:\n    pip install watchdog\n\ncelery: watchmedo\n    watchmedo auto-restart --directory=./ --pattern=*.py --recursive -- celery -A my-project worker  -l INFO\nYou can see how it simplifies your life by creating simple shortcuts of bash commands. It even allows to set dependencies between commands, like in the example above where celery task is not run until watchdog is installed.\n\n\n\n\n\n\nWhat is .PHONY?\n\n\n\n\n\nBut you can already notice some strange things: what is that .PHONY thing? And why does it have * in it? The key is rooted in the original purpose of Makefiles - to compile C programs. When you build some C program, you need to specify all the files that are needed for building. So you write smth like this:\n\nprog: file1.o file2.o file3.o\n    gcc -o prog file1.o file2.o file3.o\nThe case is, each building target is always a file or directory. If dependency target file already exists - you don’t need to rerun corresponding command, and make skips it.\nBut when you’re using Makefile just for arbitrary commandline shortcuts, this behavior is usually unwanted.\n.PHONY is used to tell make that targets listed there are not real files, but just labels. So make lint will always run lint command, even if there is a file named lint in the current directory.\n\n\n\nMore information on this and other tips of adapting Makefiles for Python projects you can find here.\nAnother example of a bit more advanced Makefile target with parameter:\n\nlist_all_tests:\n    find . -name '*.py' -not -path \"*venv*\" | grep test | sed 's|\\.py||g' | sed 's|^\\./||g' | sed 's|/|.|g'\n\ntest: # it has a parameter s\n    make list_all_tests | grep $(s) | xargs --no-run-if-empty --verbose ./manage.py test --keepdb --failfast\nAs you can see, test target has a parameter s, which is a substring of the test name you want to run. E.g., in order to find and run all test related to authentication, assuming their filenames contain auth, you need to call make test s=auth."
  },
  {
    "objectID": "posts/just-make/just-make.html#just",
    "href": "posts/just-make/just-make.html#just",
    "title": "Just Make",
    "section": "Just",
    "text": "Just\nJust is a simpler and more modern alternative to Make. It also avoids some of its pitfalls, making it useful for a broader range of tasks. But basic syntax is the same, you can just copy it from Makefile to justfile and simple commands will work out of the box.\nIt has a good documentaion.\n\n\n\n\n\n\nParameters\n\n\n\n\n\nE.g., this is how to use parameters for our test example above:\n\nlist_all_tests:\n    find . -name '*.py' -not -path \"*venv*\" | grep test | sed 's|\\.py||g' | sed 's|^\\./||g' | sed 's|/|.|g'\n\ntest substring: # it has a parameter substring\n    make list_all_tests | grep {{substring}} | xargs --no-run-if-empty --verbose ./manage.py test --keepdb --failfast\nAnd to run it you need to call just test auth. Note, that you should not write the name of the parameter when invoking the command, which would be annoying in case of full parameter name.\nParameters can have default values. This is how I use it to create new draft posts:\ndefault_ext := 'ipynb'\n\ndraft name ext=default_ext: # just first param \n    mkdir posts/{{name}} && \\\n    cp -n _examples/new_draft.{{ext}} posts/{{name}}/{{name}}.{{ext}}\nAnd to run it:\njust draft my-new-post # default extension is ipynb\njust draft my-new-post md # markdown is used\n\n\n\n\n\n\n\n\n\nEnv files\n\n\n\n\n\nAnother perk of Just is that it allows to use variables from .env file:\nset dotenv-filename := \".env\"\n\nprint_region:\n    echo $AWS_REGION\n\nlist_buckets:\n    aws s3 ls"
  },
  {
    "objectID": "posts/just-make/just-make.html#install-macos",
    "href": "posts/just-make/just-make.html#install-macos",
    "title": "Just Make",
    "section": "Install (MacOS)",
    "text": "Install (MacOS)\nIt’s available on most of the platforms, I’ll just describe my case. The first is to install on your system, and it’s available via Homebrew:\nbrew install just\nThen you want have autocompletion for just in your shell. In case of zsh on MacOS I needed to follow this instruction add to my ~/.zshrc file:\n# Init Homebrew, which adds environment variables\neval \"$(brew shellenv)\"\n\nfpath=($HOMEBREW_PREFIX/share/zsh/site-functions $fpath)\nin the beggining of the file.\nThen you need to restart your terminal or run source .zshrc and you should be ready to go.\nAlso, I’m using VSCode, and extensions are available for Just as well."
  },
  {
    "objectID": "posts/on_postgres_concurrency/index.html",
    "href": "posts/on_postgres_concurrency/index.html",
    "title": "On Postgres Concurrency",
    "section": "",
    "text": "Figure 1: Counter - lost update\n\n\n\n\n1 Experimental environment\nTo check behaviour of postgres, we will run the official docker container, and create some tables. We will use sqlalchemy ORM for working with that.\n\n# !docker run -d --name pg-concurrency -e POSTGRES_PASSWORD=mysecretpassword -p 2345:5432 postgres\n\n\n\nTables used in examples\nfrom sqlalchemy import create_engine, insert, select, text, Integer, String, Text\nfrom sqlalchemy.orm import declarative_base, mapped_column, Session\nimport pytest\n\nPG_URL = 'postgresql+psycopg2://postgres:mysecretpassword@localhost:2345/postgres'\nengine = create_engine(PG_URL)\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = 'users'\n\n    id = mapped_column(Integer, primary_key=True)\n    name = mapped_column(String, nullable=True)\n\n\nclass Product(Base):\n    __tablename__ = 'product'\n    \n    id = mapped_column(Integer, primary_key=True)\n    name = mapped_column(String, nullable=True)\n    num_likes = mapped_column(Integer, default=0)\n\n    def __str__(self):\n        return f'p#{self.id}'\n\nBase.metadata.drop_all(engine)\nBase.metadata.create_all(engine)\n\n\n\n\nUtilities used in examples\nses_autocommit = Session(\n    engine.execution_options(isolation_level=\"AUTOCOMMIT\"), \n    autoflush=True, autobegin=True\n)\n\n# Clean table\ndef clean_table():\n    ses_autocommit.execute(text(\"DELETE from product\"))#; session.commit()\n    ses_autocommit.execute(text(\"DELETE from users\"))#; session.commit()\nfrom sqlalchemy.orm import sessionmaker\n\nReadCommittedSession = sessionmaker(create_engine(PG_URL, isolation_level='READ COMMITTED'), autoflush=True)\nRepeatableReadSession = sessionmaker(create_engine(PG_URL, isolation_level=\"REPEATABLE READ\"), autoflush=True)\nSerializableSession = sessionmaker(create_engine(PG_URL, isolation_level=\"SERIALIZABLE\"), autoflush=True)\n\n\n\n\n2 Bare minimum - Repeatable Reads (RC) level\nThe minimal isolation level, which provides only basic guarantee - nobody sees changes produced by a transaction unless it’s committed. This is called, “dirty reads” prevented. Otherwise it would be a nightmare, isn’t it?\n\n\n\n\n\n\nNote\n\n\n\nCould we go below this level, and database would allow dirty reads? In SQl standard there’s a level called “READ UNCOMMITTED” which sits below RC and suppose to do exactly this, but Postgres developers decided to not implement it. More correctly, it presents but behaves exactly as RC. In the docs you can find the reasons for that: “This is because it is the only sensible way to map the standard isolation levels to PostgreSQL’s multiversion concurrency control architecture.”. But actually I’m thinking it’s just not very helful for anything. I can barely imaging a system which would tolerate repeatable reads (as well as “repeatable writes”). This is basically an auto-commit mode with possibility to revert all changes back. Could be useful for something?\n\n\nSo, when you’re running in RC, you can be sure that nobody will see your changes before (and if) you commit. But which changes you see? The answer is, on each operation you see:\n\nyour current changes (made by previous operations)\nthe most fresh global state of db (of course, comitted)\n\nIt means, if some transaction committed in between your subsequent queries, it will immediately affect you on the next query (or commit).\nThese two behaviour aspects are illustrated in the following code:\n\n\nRC guarantees in action\nclean_table()\n\nwith (\n    ReadCommittedSession() as Alice,\n    ReadCommittedSession() as Bob\n):\n    # 1\n    Alice.add(\n        Product(name='inserted by Alice')\n    )\n\n    # 2\n    found = Bob.query(Product).filter(Product.name=='inserted by Alice').all()\n    assert not found, \"Dirty read prevented !\"\n    \n    # 3\n    Alice.commit()\n\n    # 4: the same query now gives another result - this is called \"non-repeatable reads\"\n    # which is fine for RC level\n    found = Bob.query(Product).filter(Product.name=='inserted by Alice').all()\n    assert found, \"This is non-repeatable read :(\"\n\n\n\n\n3 Non-consistent snapshot\nYou may think, RR provides fair conditions? The second rule sounds even attractive - you’re working always with an up-to-date data. If something changes in db, you immediately sees it. Your code should just tolerate the case of “non-repeatable read” (or “phantom read”, there’re small differencies between these two anomalies, but essentially it’s the same): you run some query in the beginning of transaction, later repeat exactly the same query, and the result could be different. Maybe you’re working with some record, and your next operation is an update of this record; but another transaction just deleted this record in meanwhile; your code should not be broken because of that suprise.\nIf it doesn’t confuse you - all seems good.\nWhen it’s a not acceptable ?\nWhen you’re running a kind of snapshot of database state, and you need a consistent view of database.\nExample:\nImagine banking system with 2 tables - Cards and Accounts. You going to make a report of current state of these 2 tables. The following transaction running in RC will provide you an inconsistent report:\n\nYou dump (SELECT * FROM Accounts) all the accounts\nSome transaction creates a new card #N and account attached to it.\nYou dump all cards\n\nResult: The resulting report have broken link: a card #N with non-existing account.\n\n\n4 Need for locks\nBut another thing missed in RC is any kind of automatic locking, which help you to tackle concurrent data modifications.\nThe first example is lost update, occuring with quite standard Read-Modify-Write pattern. Imaging Accounts table, and Alice wants to send 10$ to Carl. The same wants to do Bob, and Carl should expect increase of 20$ on their balance. Both Bob and Alice runs their transactions in RC mode:\n\nAlice reads account of Carl and observes the current balance of 100$\nBob does the same: reads account of Carl and observes the current balance of 100$\nAlice adds 10 to current balance of 100, and updates Carl record with value of 110$. Commits.\nBob does the same: adds 10 to current balance of 100, and updates Carl record with value of 110$. Commits\n\nThe last update of Bob overrides that one of Alice, and the resulting balance of Carl is only 110$.\nYou can say it’s quite naive to make increments on client side, and that could be fixed with database increments (...SET balance = balance + 10). While it helps in this case, it’s not a remedy in general situation.\nHere’s another example:\nNow you’re going to make a transfer from account “Alice”, but the only thing you should check beforehand is that account has enough money. Your transaction runs in RC mode:\n\nYou read Alice balance - let it be 100. It’s more that required 80$, so proceeding.\nSome other transaction decreases balance to 50$.\nYou’re decreasing the balance by 80$ (with help of UPDATE balance=balance-80 WHERE ...).\nCommit.\n\nNothing prevents you to commit, process inapropriate spending. In result, Alice balance is negative. You could fix this exact case by imposing constrains on column “balance”, but there’s no general treatment.\nAllright, all such cases should be resolved with locks. Reminding, there’re two types of locks:\n\npessimistic locking, with help of real, explicit locks (SELECT FOR UPDATE)1\noptimistic locking, enforced by higher isolation levels\n\n1 Such locks has performance advantages in case of frequent contention. But a lot of disadvages, such as: 1. you need to take care of them manually, which complicates development 2. possibility of deadlocks 3. poor performance in case of extensive usage with low chance of conflictSo, staying on RC level, you can fallback to explicit locks in Read-Modify-Write cycle, or move to higher isolation level to enjoy safety provided by optimistic locking.\nThe following example is also illustrated in Figure 1.\n\n# Example of lost update\n\nclean_table()\n\nproduct = Product(name='common')\nses_autocommit.add(product)#; ses_autocommit.commit()\nses_autocommit.flush()\n\n\nlevel = 'READ COMMITTED'\n# level = 'REPEATABLE READ'\nwith (\n    Session(create_engine(PG_URL, isolation_level = level)) as session_A,\n    Session(create_engine(PG_URL, isolation_level = level)) as session_B\n):\n        # 1\n        pA = session_A.get(Product, product.id)\n        assert pA.num_likes == 0 \n\n        # 2\n        pB = session_B.get(Product, product.id)\n        assert pB.num_likes == 0         \n        \n        # 3\n        pA.num_likes += 1\n        session_A.commit()\n\n        # 4\n        pB.num_likes += 1\n        session_B.commit()\n\nses_autocommit.refresh(product)\n\n# 5\nassert product.num_likes == 1, \"Update is lost\"\n\n\n\n\n5 Golden mean - Repeatable Reads (RR) level\nRR level adds the following policies on top of RC:\n\nyou’re working with the same database state (snapshot) from the beginning of the transaction to the end (commit)\noptimistic locking is set for the records you modify\n\nWhen transaction begins, current database snapshot2 is taking and you stay within it up to the end, this is why this level also called Snapshot Isolation. As we already discussed, it’s a neccesary condition for snapshots/reports generation, including full database dumps and system snapshots allowing to later restore.\n2 Internally it’s achieved through MVCC mechanism. It’s quite similar to Git - transaction starts its own branch with later attempt to merge back into master.\n# Prevention of non-repeatable read example\n\nclean_table()\n\nwith (\n    Session(create_engine(PG_URL)) as session_A,\n    Session(create_engine(PG_URL, isolation_level = \"REPEATABLE READ\")) as session_B\n):\n        # 1\n        session_A.add(Product(name='from A'))\n\n        # 2\n        found = session_B.query(Product).filter(Product.name=='from A').all()\n        assert not found, \"Dirty read prevented :)\"\n        # 3\n        session_A.commit()\n\n        # 4\n        found = session_B.query(Product).filter(Product.name=='from A').all()\n        assert not found, \"Non-repeatable read prevented :)\"\n\nBut probably more important feature provided by RR - optimistic locking on the data you write.\nThis way you handle a lot of concurrency issues, exactly concurrent modifying of the same records.\n\nimport pytest\n\nclean_table()\n\nproduct = Product(name='common')\nses_autocommit.add(product); ses_autocommit.flush()\n\n\nwith (\n    ReadCommittedSession() as Alice,  # This transaction could be on any level\n    RepeatableReadSession() as Bob\n):\n        # 1\n        pA = Alice.get(Product, product.id)\n        assert pA.num_likes == 0 \n\n        # 2\n        pB = Bob.get(Product, product.id)\n        assert pB.num_likes == 0         \n        \n        # 3\n        pA.num_likes += 1\n        Alice.commit()\n\n        # 4: on this step, transaction was failed to commit, and lost update was prevented\n        Bob.refresh(pB)\n        assert pB.num_likes == 0, \"Repeatable read enforced\"\n        pB.num_likes += 1\n\n        with pytest.raises(Exception) as excinfo:\n            Bob.commit()\n        assert (\n              \"could not serialize access due to concurrent update\" in str(excinfo.value)\n        )\n\nses_autocommit.refresh(product)\n\n# 5\nassert product.num_likes == 1, \"Second update was reverted\"\n\n\n\nHow granular is the optimistic locking provided by RR? It has the same granularity as pessimistic ones (SELECT FOR UPDATE Products WHERE id=1). It means, you lock the whole row. Despite the other transactions could only modify another column of the row, you will not be able to commit. Technically, there will be no write conflicts or lost updates. Here’s an example:\n\n\nclean_table()\n\nproduct = Product(name='common')\nses_autocommit.add(product); ses_autocommit.flush()\n\n\nwith (\n    ReadCommittedSession() as Alice,  # This transaction could be on any level\n    RepeatableReadSession() as Bob\n):\n        # 1\n        pA = Alice.get(Product, product.id)\n        assert pA.num_likes == 0 \n\n        # 2\n        pB = Bob.get(Product, product.id)\n        assert pB.num_likes == 0         \n        \n        # 3 Alice modifies another column - name\n        pA.name = \"Alice's favourite product\"\n        Alice.commit()\n\n        # 4: on this step, transaction was failed to commit, and lost update was prevented\n        \n        pB.num_likes += 1\n        with pytest.raises(Exception) as excinfo:\n            Bob.commit()\n        assert (\n              \"could not serialize access due to concurrent update\" in str(excinfo.value)\n        )\n\nses_autocommit.refresh(product)\n\n# 5\nassert product.name == \"Alice's favourite product\", \"Effect of Alice's transaction\"\nassert product.num_likes == 0, \"Bob's transaction was not commited\"\n\n\n\nAs we see, RR is a powerful concurrency mechanism, capable to solve the cases of modifying the same records (rows). But what if there’re no row to impose lock onto? Basically, we’re talking about the case covered by whole-table locks: we want to enforce some business rule on table level, e.g. uniqueness of some type of records.\n\nclean_table()\n\nlevel = 'REPEATABLE READ'\n\nunique_name = 'Unique'\nwith (\n    Session(create_engine(PG_URL, isolation_level = level)) as session_A,\n    Session(create_engine(PG_URL, isolation_level = level)) as session_B\n):\n    #1\n    found = session_A.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"A decides the name is not taken\"\n\n    session_A.add(Product(name=unique_name))\n\n    #2\n    found = session_B.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"B decides the name is not taken\"\n\n    session_B.add(Product(name=unique_name))\n\n    #3\n    session_A.commit()\n\n    session_B.commit()\n\n\nfound = session_A.query(Product).filter(Product.name == unique_name).all()\nassert len(found) == 2, \"Uniqness checking logic was bypassed :(\"\n\n\n\nIn such cases, you can fallback to use of table-wide explicit locks.\n\n\n6 Serializable (S) - too safe to be true\nStandard formulation: level S guarantees that if transaction commit is accepted, there’s an order of transactions which gives the same result if they are running one by one, serially. Which basically means - you can think there’s no concurrency at all, no concurrency anomalies could happen3.\n3 Of course - for those transactions which were commited. Basically, the only concurrency anomaly you can face - commit rejected.4 Which is called predicate locksBut I like to think about this level as an addon to RR, which adds an additional optimistic locks on read queries (and bulk update) 4. In addition to checking for the records you modify (which is available on RR too), you check for all the records you have read or could have read, in case they were already commited.\nIf you read some set of records in the beginning of transaction, and before your commit some other transaction did something which changes the result of the same query if it would be repeated now - your transaction will be rejected. It makes a lot of sense - your logic could depend on previous read results, and your actions would be different now, when data was changed.\nLet’s look how it solves previous example of checking for table-wide uniqueness:\n\nclean_table()\n\nunique_name = 'Unique'\n\n\n# Both transactions should be serial. !!\nwith (\n    SerializableSession() as Alice,\n    SerializableSession() as Bob\n):\n    #1 Alice checks the name is not taken, and takes it\n    found = Alice.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"Alice decides the name is not taken\"\n\n    Alice.add(Product(name=unique_name))\n\n    #2 Bob does the same\n    found = Bob.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"Bob decides the name is not taken\"\n\n    Bob.add(Product(name=unique_name))\n\n    #3\n    Alice.commit()\n\n    #4\n    with pytest.raises(Exception) as excinfo:\n        Bob.commit()\n    assert (\n        \"could not serialize access due to read/write dependencies among transactions\" in str(excinfo.value)\n    )\n\nfound = ses_autocommit.query(Product).filter(Product.name == unique_name).all()\nassert len(found) == 1, \"Uniqness checking logic was not bypassed! :)\"\n\n\nHow technically this is implemented?\nSo called predicate locks are used. Conceptually, on each your SELECT query, the predicate is saved (basically, WHERE condition, defining the set of rows returned). They doesn’t really lock anything, but just are used to find out dependencies between transactions. If it’s not possible to reorder and serialize them (which means, cycles present), a transaction (the last one) is rejected.\nThis is a difference from optimistic row-level locks, available at RR, where last transaction is always rejected. In the case predicate logs, Postgres will try to reorder transactions, and only if it’s not possible - reject one.\nIn the following example, you see a reordering of transactions. Alice already created a product and commited by the time Bob takes a decision based on already outdated information. Despite that, it’s possible to reorder transactions (first Bob, then Alice) making both Alice’c and Bob’s actions consistent.5 This is quite fair: which transaction would be committed first, Alice or Bob, is anyway a random choice.\n5 The subtle difference from the previous example is only in the step 2. When Bob creates record in products table, it makes transactions non-serializable. When there’s a write to users table - all good.\nclean_table()\n\nunique_name = 'Unique'\n\n\n# Both transactions should be serial. !!\nwith (\n    SerializableSession() as Alice,\n    SerializableSession() as Bob\n):\n    #1 Alice checks the name is not taken, and takes it\n    found = Alice.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"Alice decides the name is not taken\"\n\n    Alice.add(Product(name=unique_name))\n\n    #2 Bob does the same\n    found = Bob.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"Bob decides the name is not taken\"\n\n    Bob.add(\n        User(name=f\"I'm Bob who observed {len(found)} products in db.\")\n    )\n    \n    #3\n    Alice.commit()\n\n    #4\n\n    Bob.commit()\n\n\nfound = ses_autocommit.query(Product).filter(Product.name == unique_name).all()\nassert len(found) == 1, \"Uniqness checking logic was not bypassed! :)\"\n\nassert (\n    ses_autocommit.query(User).first().name \n    == \"I'm Bob who observed 0 products in db.\"\n)\n\nAn important thing to clarify: both transactions should be of Serializable level to enforce this behaviour. In other words - those checks works only between serializable transactions. If some transaction on lower level changes the data that you read inside Serializable, nothing will prevent you to commit (and probably make an error).\n\n## Case of one transaction of lower level\n\nclean_table()\n\n# Both transactions should be serial. !!\nwith (\n    SerializableSession() as Alice,\n    ReadCommittedSession() as Bob  \n):\n    #1\n    read_products = list(Alice.query(Product).all() )\n    print(read_products)\n\n    # 2\n    Bob.add(Product(name='this product is something new, not seen by Alice yet'))\n    Bob.commit()\n    # 3\n    \n    Alice.add(Product(name='something'))\n    Alice.commit()\n\nassert ses_autocommit.query(Product).count() == 2, \"both transactions accepted\"\n\n[]\n\n\n\n\n7 Summary - mental model\nLet’s summarize isolation levels of Postgres. This is a table taken from official postgres documention: \nI propose an alternative model of thinking about these levels, which I find more simple and easier to remember.\n\n\n\nTable 1: My model of isolation levels\n\n\n\n\n\n\n\n\n\n\n\nlevel\nWhich data you can see/modify?6\nOptimistic locking?\nWhat happens on commit?\n\n\n\n\nRC\nFresh current state, committed so far\nNo\n_\n\n\nRR\nSnapshot - comitted by the start of tr.\nOn records you modified\nAbort, if conflict detected\n\n\nS\nSnapshot - comitted by the start of tr.\nPlus, on predicates you used (either in SELECT or bulk UPDATE)\nTry to reorder transactions, if not possible - abort the last one\n\n\n\n6 Besides created by me during this transaction execution."
  },
  {
    "objectID": "posts/ssh_key_add/ssh_key_add.html",
    "href": "posts/ssh_key_add/ssh_key_add.html",
    "title": "How to enable SSH for your git",
    "section": "",
    "text": "Generate SSH key\nAssuming you have OpenSSH installed, either on Linux or MacOS. Basically, that’s all you need to set-up secure ssh connection to your Github/Gitlab/Bitbucket repositories:\ncd ~/.ssh\nssh-keygen -t ed25519 -b 4096 -f MY_GITLAB # optional:  -C \"MY@EMAIL.com\" \nssh-add MY_GITLAB  # this file is created in the same folder, along with MY_GITLAB.pub\nFinal step is to take public key stored in ~/.ssh/MY_GITLAB.pub and paste it into Github/Gitlab/Bitbucket settings.\n\n\nAgent forwarding\nSometimes you’re going to ssh into remote server and then run git pull which will ask you for your password.\nTo avoid that you can enable so-called agent forwarding:\nssh -A user@remote_server\nIn this connection mode, your local ssh key will be used to authenticate you on remote server, and git clone will just work there.\nBut, be aware of security implications. Good article on that topic is here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal notes on common topics",
    "section": "",
    "text": "How to enable SSH for your git\n\n\n\n\n\n\nsmalltips\n\n\n\nThose 2 commands is all you need\n\n\n\n\n\nSep 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOn Postgres Concurrency\n\n\n\n\n\n\nprogramming\n\n\nsoftware architecture\n\n\n\nSeveral concurrency patters presented \n\n\n\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nJust Make\n\n\n\n\n\n\nsmalltips\n\n\n\nMake is good, but there’s just a better alternative. \n\n\n\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCompare pandas ignoring NAN\n\n\n\n\n\n\npandas\n\n\nsmalltips\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\n\n\n\n\nNo matching items"
  }
]