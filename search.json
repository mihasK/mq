[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/on_postgres_concurrency/index.html",
    "href": "posts/on_postgres_concurrency/index.html",
    "title": "On Postgres Concurrency",
    "section": "",
    "text": "Figure 1: Counter - lost update\n\n\n\n\n1 Experimental environment\nTo check behaviour of postgres, we will run the official docker container, and create some tables. We will use sqlalchemy ORM for working with that.\n\n# !docker run -d --name pg-concurrency -e POSTGRES_PASSWORD=mysecretpassword -p 2345:5432 postgres\n\n\n\nTables used in examples\nfrom sqlalchemy import create_engine, insert, select, text, Integer, String, Text\nfrom sqlalchemy.orm import declarative_base, mapped_column, Session\nimport pytest\n\nPG_URL = 'postgresql+psycopg2://postgres:mysecretpassword@localhost:2345/postgres'\nengine = create_engine(PG_URL)\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = 'users'\n\n    id = mapped_column(Integer, primary_key=True)\n    name = mapped_column(String, nullable=True)\n\n\nclass Product(Base):\n    __tablename__ = 'product'\n    \n    id = mapped_column(Integer, primary_key=True)\n    name = mapped_column(String, nullable=True)\n    num_likes = mapped_column(Integer, default=0)\n\n    def __str__(self):\n        return f'p#{self.id}'\n\nBase.metadata.drop_all(engine)\nBase.metadata.create_all(engine)\n\n\n\n\nUtilities used in examples\nses_autocommit = Session(\n    engine.execution_options(isolation_level=\"AUTOCOMMIT\"), \n    autoflush=True, autobegin=True\n)\n\n# Clean table\ndef clean_table():\n    ses_autocommit.execute(text(\"DELETE from product\"))#; session.commit()\n    ses_autocommit.execute(text(\"DELETE from users\"))#; session.commit()\nfrom sqlalchemy.orm import sessionmaker\n\nReadCommittedSession = sessionmaker(create_engine(PG_URL, isolation_level='READ COMMITTED'), autoflush=True)\nRepeatableReadSession = sessionmaker(create_engine(PG_URL, isolation_level=\"REPEATABLE READ\"), autoflush=True)\nSerializableSession = sessionmaker(create_engine(PG_URL, isolation_level=\"SERIALIZABLE\"), autoflush=True)\n\n\n\n\n2 Bare minimum - Repeatable Reads (RC) level\nThe minimal isolation level, which provides only basic guarantee - nobody sees changes produced by a transaction unless it’s committed. This is called, “dirty reads” prevented. Otherwise it would be a nightmare, isn’t it?\n\n\n\n\n\n\nNote\n\n\n\nCould we go below this level, and database would allow dirty reads? In SQl standard there’s a level called “READ UNCOMMITTED” which sits below RC and suppose to do exactly this, but Postgres developers decided to not implement it. More correctly, it presents but behaves exactly as RC. In the docs you can find the reasons for that: “This is because it is the only sensible way to map the standard isolation levels to PostgreSQL’s multiversion concurrency control architecture.”. But actually I’m thinking it’s just not very helful for anything. I can barely imaging a system which would tolerate repeatable reads (as well as “repeatable writes”). This is basically an auto-commit mode with possibility to revert all changes back. Could be useful for something?\n\n\nSo, when you’re running in RC, you can be sure that nobody will see your changes before (and if) you commit. But which changes you see? The answer is, on each operation you see:\n\nyour current changes (made by previous operations)\nthe most fresh global state of db (of course, comitted)\n\nIt means, if some transaction committed in between your subsequent queries, it will immediately affect you on the next query (or commit).\nThese two behaviour aspects are illustrated in the following code:\n\n\nRC guarantees in action\nclean_table()\n\nwith (\n    ReadCommittedSession() as Alice,\n    ReadCommittedSession() as Bob\n):\n    # 1\n    Alice.add(\n        Product(name='inserted by Alice')\n    )\n\n    # 2\n    found = Bob.query(Product).filter(Product.name=='inserted by Alice').all()\n    assert not found, \"Dirty read prevented !\"\n    \n    # 3\n    Alice.commit()\n\n    # 4: the same query now gives another result - this is called \"non-repeatable reads\"\n    # which is fine for RC level\n    found = Bob.query(Product).filter(Product.name=='inserted by Alice').all()\n    assert found, \"This is non-repeatable read :(\"\n\n\n\n\n3 Non-consistent snapshot\nYou may think, RR provides fair conditions? The second rule sounds even attractive - you’re working always with an up-to-date data. If something changes in db, you immediately sees it. Your code should just tolerate the case of “non-repeatable read” (or “phantom read”, there’re small differencies between these two anomalies, but essentially it’s the same): you run some query in the beginning of transaction, later repeat exactly the same query, and the result could be different. Maybe you’re working with some record, and your next operation is an update of this record; but another transaction just deleted this record in meanwhile; your code should not be broken because of that suprise.\nIf it doesn’t confuse you - all seems good.\nWhen it’s a not acceptable ?\nWhen you’re running a kind of snapshot of database state, and you need a consistent view of database.\nExample:\nImagine banking system with 2 tables - Cards and Accounts. You going to make a report of current state of these 2 tables. The following transaction running in RC will provide you an inconsistent report:\n\nYou dump (SELECT * FROM Accounts) all the accounts\nSome transaction creates a new card #N and account attached to it.\nYou dump all cards\n\nResult: The resulting report have broken link: a card #N with non-existing account.\n\n\n4 Need for locks\nBut another thing missed in RC is any kind of automatic locking, which help you to tackle concurrent data modifications.\nThe first example is lost update, occuring with quite standard Read-Modify-Write pattern. Imaging Accounts table, and Alice wants to send 10$ to Carl. The same wants to do Bob, and Carl should expect increase of 20$ on their balance. Both Bob and Alice runs their transactions in RC mode:\n\nAlice reads account of Carl and observes the current balance of 100$\nBob does the same: reads account of Carl and observes the current balance of 100$\nAlice adds 10 to current balance of 100, and updates Carl record with value of 110$. Commits.\nBob does the same: adds 10 to current balance of 100, and updates Carl record with value of 110$. Commits\n\nThe last update of Bob overrides that one of Alice, and the resulting balance of Carl is only 110$.\nYou can say it’s quite naive to make increments on client side, and that could be fixed with database increments (...SET balance = balance + 10). While it helps in this case, it’s not a remedy in general situation.\nHere’s another example:\nNow you’re going to make a transfer from account “Alice”, but the only thing you should check beforehand is that account has enough money. Your transaction runs in RC mode:\n\nYou read Alice balance - let it be 100. It’s more that required 80$, so proceeding.\nSome other transaction decreases balance to 50$.\nYou’re decreasing the balance by 80$ (with help of UPDATE balance=balance-80 WHERE ...).\nCommit.\n\nNothing prevents you to commit, process inapropriate spending. In result, Alice balance is negative. You could fix this exact case by imposing constrains on column “balance”, but there’s no general treatment.\nAllright, all such cases should be resolved with locks. Reminding, there’re two types of locks:\n\npessimistic locking, with help of real, explicit locks (SELECT FOR UPDATE)1\noptimistic locking, enforced by higher isolation levels\n\n1 Such locks has performance advantages in case of frequent contention. But a lot of disadvages, such as: 1. you need to take care of them manually, which complicates development 2. possibility of deadlocks 3. poor performance in case of extensive usage with low chance of conflictSo, staying on RC level, you can fallback to explicit locks in Read-Modify-Write cycle, or move to higher isolation level to enjoy safety provided by optimistic locking.\nThe following example is also illustrated in Figure 1.\n\n# Example of lost update\n\nclean_table()\n\nproduct = Product(name='common')\nses_autocommit.add(product)#; ses_autocommit.commit()\nses_autocommit.flush()\n\n\nlevel = 'READ COMMITTED'\n# level = 'REPEATABLE READ'\nwith (\n    Session(create_engine(PG_URL, isolation_level = level)) as session_A,\n    Session(create_engine(PG_URL, isolation_level = level)) as session_B\n):\n        # 1\n        pA = session_A.get(Product, product.id)\n        assert pA.num_likes == 0 \n\n        # 2\n        pB = session_B.get(Product, product.id)\n        assert pB.num_likes == 0         \n        \n        # 3\n        pA.num_likes += 1\n        session_A.commit()\n\n        # 4\n        pB.num_likes += 1\n        session_B.commit()\n\nses_autocommit.refresh(product)\n\n# 5\nassert product.num_likes == 1, \"Update is lost\"\n\n\n\n\n5 Golden mean - Repeatable Reads (RR) level\nRR level adds the following policies on top of RC:\n\nyou’re working with the same database state (snapshot) from the beginning of the transaction to the end (commit)\noptimistic locking is set for the records you modify\n\nWhen transaction begins, current database snapshot2 is taking and you stay within it up to the end, this is why this level also called Snapshot Isolation. As we already discussed, it’s a neccesary condition for snapshots/reports generation, including full database dumps and system snapshots allowing to later restore.\n2 Internally it’s achieved through MVCC mechanism. It’s quite similar to Git - transaction starts its own branch with later attempt to merge back into master.\n# Prevention of non-repeatable read example\n\nclean_table()\n\nwith (\n    Session(create_engine(PG_URL)) as session_A,\n    Session(create_engine(PG_URL, isolation_level = \"REPEATABLE READ\")) as session_B\n):\n        # 1\n        session_A.add(Product(name='from A'))\n\n        # 2\n        found = session_B.query(Product).filter(Product.name=='from A').all()\n        assert not found, \"Dirty read prevented :)\"\n        # 3\n        session_A.commit()\n\n        # 4\n        found = session_B.query(Product).filter(Product.name=='from A').all()\n        assert not found, \"Non-repeatable read prevented :)\"\n\nBut probably more important feature provided by RR - optimistic locking on the data you write.\nThis way you handle a lot of concurrency issues, exactly concurrent modifying of the same records.\n\nimport pytest\n\nclean_table()\n\nproduct = Product(name='common')\nses_autocommit.add(product); ses_autocommit.flush()\n\n\nwith (\n    ReadCommittedSession() as Alice,  # This transaction could be on any level\n    RepeatableReadSession() as Bob\n):\n        # 1\n        pA = Alice.get(Product, product.id)\n        assert pA.num_likes == 0 \n\n        # 2\n        pB = Bob.get(Product, product.id)\n        assert pB.num_likes == 0         \n        \n        # 3\n        pA.num_likes += 1\n        Alice.commit()\n\n        # 4: on this step, transaction was failed to commit, and lost update was prevented\n        Bob.refresh(pB)\n        assert pB.num_likes == 0, \"Repeatable read enforced\"\n        pB.num_likes += 1\n\n        with pytest.raises(Exception) as excinfo:\n            Bob.commit()\n        assert (\n              \"could not serialize access due to concurrent update\" in str(excinfo.value)\n        )\n\nses_autocommit.refresh(product)\n\n# 5\nassert product.num_likes == 1, \"Second update was reverted\"\n\n\n\nHow granular is the optimistic locking provided by RR? It has the same granularity as pessimistic ones (SELECT FOR UPDATE Products WHERE id=1). It means, you lock the whole row. Despite the other transactions could only modify another column of the row, you will not be able to commit. Technically, there will be no write conflicts or lost updates. Here’s an example:\n\n\nclean_table()\n\nproduct = Product(name='common')\nses_autocommit.add(product); ses_autocommit.flush()\n\n\nwith (\n    ReadCommittedSession() as Alice,  # This transaction could be on any level\n    RepeatableReadSession() as Bob\n):\n        # 1\n        pA = Alice.get(Product, product.id)\n        assert pA.num_likes == 0 \n\n        # 2\n        pB = Bob.get(Product, product.id)\n        assert pB.num_likes == 0         \n        \n        # 3 Alice modifies another column - name\n        pA.name = \"Alice's favourite product\"\n        Alice.commit()\n\n        # 4: on this step, transaction was failed to commit, and lost update was prevented\n        \n        pB.num_likes += 1\n        with pytest.raises(Exception) as excinfo:\n            Bob.commit()\n        assert (\n              \"could not serialize access due to concurrent update\" in str(excinfo.value)\n        )\n\nses_autocommit.refresh(product)\n\n# 5\nassert product.name == \"Alice's favourite product\", \"Effect of Alice's transaction\"\nassert product.num_likes == 0, \"Bob's transaction was not commited\"\n\n\n\nAs we see, RR is a powerful concurrency mechanism, capable to solve the cases of modifying the same records (rows). But what if there’re no row to impose lock onto? Basically, we’re talking about the case covered by whole-table locks: we want to enforce some business rule on table level, e.g. uniqueness of some type of records.\n\nclean_table()\n\nlevel = 'REPEATABLE READ'\n\nunique_name = 'Unique'\nwith (\n    Session(create_engine(PG_URL, isolation_level = level)) as session_A,\n    Session(create_engine(PG_URL, isolation_level = level)) as session_B\n):\n    #1\n    found = session_A.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"A decides the name is not taken\"\n\n    session_A.add(Product(name=unique_name))\n\n    #2\n    found = session_B.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"B decides the name is not taken\"\n\n    session_B.add(Product(name=unique_name))\n\n    #3\n    session_A.commit()\n\n    session_B.commit()\n\n\nfound = session_A.query(Product).filter(Product.name == unique_name).all()\nassert len(found) == 2, \"Uniqness checking logic was bypassed :(\"\n\n\n\nIn such cases, you can fallback to use of table-wide explicit locks.\n\n\n6 Serializable (S) - too safe to be true\nStandard formulation: level S guarantees that if transaction commit is accepted, there’s an order of transactions which gives the same result if they are running one by one, serially. Which basically means - you can think there’s no concurrency at all, no concurrency anomalies could happen3.\n3 Of course - for those transactions which were commited. Basically, the only concurrency anomaly you can face - commit rejected.4 Which is called predicate locksBut I like to think about this level as an addon to RR, which adds an additional optimistic locks on read queries4. In addition to checking for the records you modify (which is available on RR too), you check for all the records you have read or could have read, in case they were already commited.\nIf you read some set of records in the beginning of transaction, and before your commit some other transaction did something which changes the result of the same query if it would be repeated now - your transaction will be rejected. It makes a lot of sense - you logic could depend on previous read results, and your actions would be different now, when data was changed.\nLet’s look how it solves previous example of checking for table-wide uniqueness:\n\nclean_table()\n\nunique_name = 'Unique'\n\n\n# Both transactions should be serial. !!\nwith (\n    SerializableSession() as Alice,\n    SerializableSession() as Bob\n):\n    #1 Alice checks the name is not taken, and takes it\n    found = Alice.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"Alice decides the name is not taken\"\n\n    Alice.add(Product(name=unique_name))\n\n    #2 Bob does the same\n    found = Bob.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"Bob decides the name is not taken\"\n\n    Bob.add(Product(name=unique_name))\n\n    #3\n    Alice.commit()\n\n    #4\n    with pytest.raises(Exception) as excinfo:\n        Bob.commit()\n    assert (\n        \"could not serialize access due to read/write dependencies among transactions\" in str(excinfo.value)\n    )\n\nfound = ses_autocommit.query(Product).filter(Product.name == unique_name).all()\nassert len(found) == 1, \"Uniqness checking logic was not bypassed! :)\"\n\n\nHow technically this is implemented?\nSo called predicate locks are used. Conceptually, on each your SELECT query, the predicate is saved (basically, WHERE condition, defining the set of rows returned). They doesn’t really lock anything, but just are used to find out dependencies between transactions. If it’s not possible to reorder and serialize them (which means, cycles present), a transaction (the last one) is rejected.\nThis is a difference from optimistic row-level locks, available at RR, where last transaction is always rejected. In the case predicate logs, Postgres will try to reorder transactions, and only if it’s not possible - reject one.\nIn the following example, you see a reordering of transactions. Alice already created a product and commited by the time Bob takes a decision based on already outdated information. Despite that, it’s possible to reorder transactions (first Bob, then Alice) making both Alice’c and Bob’s actions consistent.5 This is quite fair: which transaction would be committed first, Alice or Bob, is anyway a random choice.\n5 The subtle difference from the previous example is only in the step 2. When Bob creates record in products table, it makes transactions non-serializable. When there’s a write to users table - all good.\nclean_table()\n\nunique_name = 'Unique'\n\n\n# Both transactions should be serial. !!\nwith (\n    SerializableSession() as Alice,\n    SerializableSession() as Bob\n):\n    #1 Alice checks the name is not taken, and takes it\n    found = Alice.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"Alice decides the name is not taken\"\n\n    Alice.add(Product(name=unique_name))\n\n    #2 Bob does the same\n    found = Bob.query(Product).filter(Product.name == unique_name).all()\n\n    assert not found, \"Bob decides the name is not taken\"\n\n    Bob.add(\n        User(name=f\"I'm Bob who observed {len(found)} products in db.\")\n    )\n    \n    #3\n    Alice.commit()\n\n    #4\n\n    Bob.commit()\n\n\nfound = ses_autocommit.query(Product).filter(Product.name == unique_name).all()\nassert len(found) == 1, \"Uniqness checking logic was not bypassed! :)\"\n\nassert (\n    ses_autocommit.query(User).first().name \n    == \"I'm Bob who observed 0 products in db.\"\n)\n\nAn important thing to clarify: both transactions should be of Serializable level to enforce this behaviour. In other words - those checks works only between serializable transactions. If some transaction on lower level changes the data that you read inside Serializable, nothing will prevent you to commit (and probably make an error).\n\n## Case of one transaction of lower level\n\nclean_table()\n\n# Both transactions should be serial. !!\nwith (\n    SerializableSession() as Alice,\n    ReadCommittedSession() as Bob  \n):\n    #1\n    read_products = list(Alice.query(Product).all() )\n    print(read_products)\n\n    # 2\n    Bob.add(Product(name='this product is something new, not seen by Alice yet'))\n    Bob.commit()\n    # 3\n    \n    Alice.add(Product(name='something'))\n    Alice.commit()\n\nassert ses_autocommit.query(Product).count() == 2, \"both transactions accepted\"\n\n[]\n\n\n\n\n7 Summary - mental model\nLet’s summarize isolation levels of Postgres. This is a table taken from official postgres documention: \nI propose an alternative model of thinking about these levels, which I find more simple and easier to remember.\n\n\n\nTable 1: My model of isolation levels\n\n\n\n\n\n\n\n\n\n\n\nlevel\nWhich data you can see/modify?6\nOptimistic locking?\nWhat happens on commit?\n\n\n\n\nRC\nFresh current state, committed so far\nNo\n_\n\n\nRR\nSnapshot - comitted by the start of tr.\nOn records you modified\nAbort, if conflict detected\n\n\nS\nSnapshot - comitted by the start of tr.\nPlus, on predicates you used (either in SELECT or bulk UPDATE)\nTry to reorder transactions, if not possible - abort the last one\n\n\n\n6 Besides created by me during this transaction execution."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on job-related topics",
    "section": "",
    "text": "On Postgres Concurrency\n\n\n\n\n\n\nprogramming\n\n\nsoftware architecture\n\n\n\nSeveral concurrency patters presented \n\n\n\n\n\nJul 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\n\n\n\n\nNo matching items"
  }
]