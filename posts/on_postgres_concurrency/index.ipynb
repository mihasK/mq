{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: On Postgres Concurrency\n",
    "title-block-banner: true\n",
    "#keywords:\n",
    "#  - Postgres\n",
    "categories: \n",
    "  - programming\n",
    "  - software architecture\n",
    "\n",
    "abstract: |\n",
    "  Several concurrency patters presented\n",
    "date: last-modified\n",
    "\n",
    "number-sections: true\n",
    "execute: \n",
    "  enabled: false\n",
    "\n",
    "reference-location: margin\n",
    "citation-location: margin\n",
    "\n",
    "crossref:\n",
    "  lof-title: \"List of Figures\"\n",
    "lightbox: true\n",
    "\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    page-layout: article\n",
    "---\n",
    "\\listoffigures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Counter - lost update](pg_conc1.png){#fig-counter-lost-update}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check behaviour of postgres, we will run the official docker container, and create some tables.\n",
    "We will use sqlalchemy ORM for working with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker run -d --name pg-concurrency -e POSTGRES_PASSWORD=mysecretpassword -p 2345:5432 postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: Tables-setup\n",
    "#| lst-cap: Y\n",
    "#| lst-label: Z\n",
    "#| code-fold: true\n",
    "#| code-summary: Tables used in examples\n",
    "\n",
    "from sqlalchemy import create_engine, insert, select, text, Integer, String, Text\n",
    "from sqlalchemy.orm import declarative_base, mapped_column, Session\n",
    "import pytest\n",
    "\n",
    "PG_URL = 'postgresql+psycopg2://postgres:mysecretpassword@localhost:2345/postgres'\n",
    "engine = create_engine(PG_URL)\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = 'users'\n",
    "\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    name = mapped_column(String, nullable=True)\n",
    "\n",
    "\n",
    "class Product(Base):\n",
    "    __tablename__ = 'product'\n",
    "    \n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    name = mapped_column(String, nullable=True)\n",
    "    num_likes = mapped_column(Integer, default=0)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'p#{self.id}'\n",
    "\n",
    "Base.metadata.drop_all(engine)\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: Utilities\n",
    "#| code-fold: true\n",
    "#| code-summary: Utilities used in examples\n",
    "\n",
    "ses_autocommit = Session(\n",
    "    engine.execution_options(isolation_level=\"AUTOCOMMIT\"), \n",
    "    autoflush=True, autobegin=True\n",
    ")\n",
    "\n",
    "# Clean table\n",
    "def clean_table():\n",
    "    ses_autocommit.execute(text(\"DELETE from product\"))#; session.commit()\n",
    "    ses_autocommit.execute(text(\"DELETE from users\"))#; session.commit()\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "ReadCommittedSession = sessionmaker(create_engine(PG_URL, isolation_level='READ COMMITTED'), autoflush=True)\n",
    "RepeatableReadSession = sessionmaker(create_engine(PG_URL, isolation_level=\"REPEATABLE READ\"), autoflush=True)\n",
    "SerializableSession = sessionmaker(create_engine(PG_URL, isolation_level=\"SERIALIZABLE\"), autoflush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bare minimum - Repeatable Reads (RC) level\n",
    "\n",
    "The minimal isolation level, which provides only basic guarantee - nobody sees changes produced by a transaction unless it's committed. This is called, __\"dirty reads\"__ prevented. Otherwise it would be a nightmare, isn't it?\n",
    "\n",
    "::: {.callout-note}\n",
    "Could we go below this level, and database would allow dirty reads?\n",
    "In SQl standard there's a level called \"READ UNCOMMITTED\" which sits below RC and suppose to do exactly this, but Postgres developers decided to not implement it. More correctly, it presents but behaves exactly as RC. In the docs you can find the reasons for that: \"This is because it is the only sensible way to map the standard isolation levels to PostgreSQL's multiversion concurrency control architecture.\". But actually I'm thinking it's just not very helful for anything. I can barely imaging a system which would tolerate repeatable reads (as well as  __\"repeatable writes\"__). This is basically an auto-commit mode with possibility to revert all changes back. Could be useful for something?\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "So, when you're running in RC, you can be sure that nobody will see your changes before (and if) you commit. But which changes you see? The answer is, on each operation you see:\n",
    "\n",
    "* your current changes (made by previous operations)\n",
    "* the most fresh global state of db (of course, comitted)\n",
    "\n",
    "It means, if some transaction committed in between your subsequent queries, it will immediately affect you on the next query (or commit).\n",
    "\n",
    "\n",
    "These two behaviour aspects are illustrated in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: RC\n",
    "#| lst-cap: Y\n",
    "#| lst-label: Z\n",
    "#| code-fold: true\n",
    "#| code-summary: RC guarantees in action\n",
    "\n",
    "clean_table()\n",
    "\n",
    "with (\n",
    "    ReadCommittedSession() as Alice,\n",
    "    ReadCommittedSession() as Bob\n",
    "):\n",
    "    # 1\n",
    "    Alice.add(\n",
    "        Product(name='inserted by Alice')\n",
    "    )\n",
    "\n",
    "    # 2\n",
    "    found = Bob.query(Product).filter(Product.name=='inserted by Alice').all()\n",
    "    assert not found, \"Dirty read prevented !\"\n",
    "    \n",
    "    # 3\n",
    "    Alice.commit()\n",
    "\n",
    "    # 4: the same query now gives another result - this is called \"non-repeatable reads\"\n",
    "    # which is fine for RC level\n",
    "    found = Bob.query(Product).filter(Product.name=='inserted by Alice').all()\n",
    "    assert found, \"This is non-repeatable read :(\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-consistent snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You may think, RR provides fair conditions? The second rule sounds even attractive - you're working always with an up-to-date data. If something changes in db, you immediately sees it. Your code should just tolerate the case of __\"non-repeatable read\"__ (or __\"phantom read\"__, there're small differencies between these two anomalies, but essentially it's the same): you run some query in the beginning of transaction, later repeat exactly the same query, and the result could be different. Maybe you're working with some record, and your next operation is an update of this record; but another transaction just deleted this record in meanwhile; your code should not be broken because of that suprise.\n",
    "\n",
    "If it doesn't confuse you - all seems good.\n",
    "\n",
    "**When it's a not acceptable ?**\n",
    "\n",
    "When you're running a kind of snapshot of database state,\n",
    "and you need a consistent view of database.\n",
    "\n",
    "Example:\n",
    "\n",
    "Imagine banking system with 2 tables - Cards and Accounts.\n",
    "You going to make a report of current state of these 2 tables.\n",
    "The following transaction running in RC will provide you an inconsistent report:\n",
    "\n",
    "1. You dump (`SELECT * FROM Accounts`) all the accounts\n",
    "2. Some transaction creates a new card #N and account attached to it.\n",
    "3. You dump all cards\n",
    "\n",
    "Result: The resulting report have broken link: a card #N with non-existing account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need for locks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But another thing missed in RC is any kind of automatic locking, which help you to tackle concurrent data modifications.\n",
    "\n",
    "The first example is __lost update__, occuring with quite standard Read-Modify-Write pattern.\n",
    "Imaging Accounts table, and Alice wants to send 10$ to Carl. The same wants to do Bob,\n",
    "and Carl should expect increase of 20$ on their balance.\n",
    "Both Bob and Alice runs their transactions in RC mode:\n",
    "\n",
    "1. Alice reads account of Carl and observes the current balance of 100$\n",
    "2. Bob does the same: reads account of Carl and observes the current balance of 100$\n",
    "3. Alice adds 10 to current balance of 100, and updates Carl record with value of 110$. Commits.\n",
    "4. Bob does the same: adds 10 to current balance of 100, and updates Carl record with value of 110$. Commits\n",
    "\n",
    "The last update of Bob overrides that one of Alice, and the resulting balance of Carl is only 110$.\n",
    "\n",
    "You can say it's quite naive to make increments on client side, and that could be fixed with database increments (`...SET balance = balance + 10`).\n",
    " While it helps in this case, it's not a remedy in general situation. \n",
    "\n",
    "Here's another example:\n",
    "\n",
    "Now you're going to make a transfer from account \"Alice\",\n",
    "but the only thing you should check beforehand is that account has enough money.\n",
    "Your transaction runs in RC mode:\n",
    "\n",
    "1. You read Alice balance - let it be 100. It's more that required 80$, so proceeding.\n",
    "2. Some other transaction decreases balance to 50$.\n",
    "3. You're decreasing the balance by 80$ (with help of `UPDATE balance=balance-80 WHERE ...`).\n",
    "4. Commit. \n",
    "\n",
    "Nothing prevents you to commit, process inapropriate spending. In result, Alice balance is negative. You could fix this exact case by imposing constrains on column \"balance\", but there's no general treatment.\n",
    "\n",
    "\n",
    "Allright, all such cases should be resolved with locks.\n",
    "Reminding, there're two types of locks:\n",
    "\n",
    "* pessimistic locking, with help of real, explicit locks (`SELECT FOR UPDATE`)[^pessimistic]\n",
    "* optimistic locking, enforced by higher isolation levels\n",
    "\n",
    "So, staying on RC level, you can fallback to explicit locks in Read-Modify-Write cycle,\n",
    "or move to higher isolation level to enjoy safety provided by optimistic locking.\n",
    "\n",
    "[^pessimistic]: Such locks has performance advantages in case of frequent contention. \n",
    "    But a lot of disadvages, such as: \n",
    "        1. you need to take care of them manually, which complicates development\n",
    "        2. possibility of deadlocks\n",
    "        3. poor performance in case of extensive usage with low chance of conflict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is also illustrated in @fig-counter-lost-update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of lost update\n",
    "\n",
    "clean_table()\n",
    "\n",
    "product = Product(name='common')\n",
    "ses_autocommit.add(product)#; ses_autocommit.commit()\n",
    "ses_autocommit.flush()\n",
    "\n",
    "\n",
    "level = 'READ COMMITTED'\n",
    "# level = 'REPEATABLE READ'\n",
    "with (\n",
    "    Session(create_engine(PG_URL, isolation_level = level)) as session_A,\n",
    "    Session(create_engine(PG_URL, isolation_level = level)) as session_B\n",
    "):\n",
    "        # 1\n",
    "        pA = session_A.get(Product, product.id)\n",
    "        assert pA.num_likes == 0 \n",
    "\n",
    "        # 2\n",
    "        pB = session_B.get(Product, product.id)\n",
    "        assert pB.num_likes == 0         \n",
    "        \n",
    "        # 3\n",
    "        pA.num_likes += 1\n",
    "        session_A.commit()\n",
    "\n",
    "        # 4\n",
    "        pB.num_likes += 1\n",
    "        session_B.commit()\n",
    "\n",
    "ses_autocommit.refresh(product)\n",
    "\n",
    "# 5\n",
    "assert product.num_likes == 1, \"Update is lost\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golden mean - Repeatable Reads (RR) level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RR level adds the following policies on top of RC:\n",
    "\n",
    "* you're working with the same database state (snapshot) from the beginning of the transaction to the end (commit)\n",
    "* optimistic locking is set for the records you modify\n",
    "\n",
    "\n",
    "When transaction begins, current database snapshot[^MVCC] is taking and you stay within it up to the end,\n",
    "this is why this level also called __Snapshot Isolation__.\n",
    "As we already discussed, it's a neccesary condition for snapshots/reports generation, including full database dumps and system snapshots allowing to later restore.\n",
    "\n",
    "[^MVCC]: Internally it's achieved through MVCC mechanism. It's quite similar to Git  - transaction starts its own branch with later attempt to merge back into master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevention of non-repeatable read example\n",
    "\n",
    "clean_table()\n",
    "\n",
    "with (\n",
    "    Session(create_engine(PG_URL)) as session_A,\n",
    "    Session(create_engine(PG_URL, isolation_level = \"REPEATABLE READ\")) as session_B\n",
    "):\n",
    "        # 1\n",
    "        session_A.add(Product(name='from A'))\n",
    "\n",
    "        # 2\n",
    "        found = session_B.query(Product).filter(Product.name=='from A').all()\n",
    "        assert not found, \"Dirty read prevented :)\"\n",
    "        # 3\n",
    "        session_A.commit()\n",
    "\n",
    "        # 4\n",
    "        found = session_B.query(Product).filter(Product.name=='from A').all()\n",
    "        assert not found, \"Non-repeatable read prevented :)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But probably more important feature provided by RR - optimistic locking on the data you write.\n",
    "\n",
    "This way you handle a lot of concurrency issues, exactly concurrent modifying of the same records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "clean_table()\n",
    "\n",
    "product = Product(name='common')\n",
    "ses_autocommit.add(product); ses_autocommit.flush()\n",
    "\n",
    "\n",
    "with (\n",
    "    ReadCommittedSession() as Alice,  # This transaction could be on any level\n",
    "    RepeatableReadSession() as Bob\n",
    "):\n",
    "        # 1\n",
    "        pA = Alice.get(Product, product.id)\n",
    "        assert pA.num_likes == 0 \n",
    "\n",
    "        # 2\n",
    "        pB = Bob.get(Product, product.id)\n",
    "        assert pB.num_likes == 0         \n",
    "        \n",
    "        # 3\n",
    "        pA.num_likes += 1\n",
    "        Alice.commit()\n",
    "\n",
    "        # 4: on this step, transaction was failed to commit, and lost update was prevented\n",
    "        Bob.refresh(pB)\n",
    "        assert pB.num_likes == 0, \"Repeatable read enforced\"\n",
    "        pB.num_likes += 1\n",
    "\n",
    "        with pytest.raises(Exception) as excinfo:\n",
    "            Bob.commit()\n",
    "        assert (\n",
    "              \"could not serialize access due to concurrent update\" in str(excinfo.value)\n",
    "        )\n",
    "\n",
    "ses_autocommit.refresh(product)\n",
    "\n",
    "# 5\n",
    "assert product.num_likes == 1, \"Second update was reverted\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How granular is the optimistic locking provided by RR?\n",
    "It has the same granularity as pessimistic ones (`SELECT FOR UPDATE Products WHERE id=1`).\n",
    "It means, you lock the whole row. Despite the other transactions could only modify another column of the row, you will not be able to commit. Technically, there will be no write conflicts or lost updates. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_table()\n",
    "\n",
    "product = Product(name='common')\n",
    "ses_autocommit.add(product); ses_autocommit.flush()\n",
    "\n",
    "\n",
    "with (\n",
    "    ReadCommittedSession() as Alice,  # This transaction could be on any level\n",
    "    RepeatableReadSession() as Bob\n",
    "):\n",
    "        # 1\n",
    "        pA = Alice.get(Product, product.id)\n",
    "        assert pA.num_likes == 0 \n",
    "\n",
    "        # 2\n",
    "        pB = Bob.get(Product, product.id)\n",
    "        assert pB.num_likes == 0         \n",
    "        \n",
    "        # 3 Alice modifies another column - name\n",
    "        pA.name = \"Alice's favourite product\"\n",
    "        Alice.commit()\n",
    "\n",
    "        # 4: on this step, transaction was failed to commit, and lost update was prevented\n",
    "        \n",
    "        pB.num_likes += 1\n",
    "        with pytest.raises(Exception) as excinfo:\n",
    "            Bob.commit()\n",
    "        assert (\n",
    "              \"could not serialize access due to concurrent update\" in str(excinfo.value)\n",
    "        )\n",
    "\n",
    "ses_autocommit.refresh(product)\n",
    "\n",
    "# 5\n",
    "assert product.name == \"Alice's favourite product\", \"Effect of Alice's transaction\"\n",
    "assert product.num_likes == 0, \"Bob's transaction was not commited\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, RR is a powerful concurrency mechanism, capable to solve the cases of modifying the same records (rows).\n",
    "But what if there're no row to impose lock onto?\n",
    "Basically, we're talking about the case covered by whole-table locks:\n",
    "we want to enforce some business rule on table level, e.g. uniqueness of some type of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_table()\n",
    "\n",
    "level = 'REPEATABLE READ'\n",
    "\n",
    "unique_name = 'Unique'\n",
    "with (\n",
    "    Session(create_engine(PG_URL, isolation_level = level)) as session_A,\n",
    "    Session(create_engine(PG_URL, isolation_level = level)) as session_B\n",
    "):\n",
    "    #1\n",
    "    found = session_A.query(Product).filter(Product.name == unique_name).all()\n",
    "\n",
    "    assert not found, \"A decides the name is not taken\"\n",
    "\n",
    "    session_A.add(Product(name=unique_name))\n",
    "\n",
    "    #2\n",
    "    found = session_B.query(Product).filter(Product.name == unique_name).all()\n",
    "\n",
    "    assert not found, \"B decides the name is not taken\"\n",
    "\n",
    "    session_B.add(Product(name=unique_name))\n",
    "\n",
    "    #3\n",
    "    session_A.commit()\n",
    "\n",
    "    session_B.commit()\n",
    "\n",
    "\n",
    "found = session_A.query(Product).filter(Product.name == unique_name).all()\n",
    "assert len(found) == 2, \"Uniqness checking logic was bypassed :(\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such cases, you can fallback to use of table-wide explicit locks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializable (S) - too safe to be true\n",
    "\n",
    "Standard formulation: level S guarantees that if transaction commit is accepted, **there's an order of transactions which gives the same result** if they are running one by one, serially. Which basically means - you can think there's no concurrency at all, no concurrency anomalies could happen[^Reject].\n",
    "\n",
    "[^Reject]: Of course - for those transactions which were commited.\n",
    "Basically, the only concurrency anomaly you can face - commit rejected.\n",
    "\n",
    "\n",
    "\n",
    "But I like to think about this level as an addon to RR, which adds **an additional optimistic locks on read queries (and bulk update) **[^predicate-lock]. In addition to checking for the records you modify (which is available on RR too), you check for all the records you have read or could have read, in case they were already commited.\n",
    "\n",
    "[^predicate-lock]: Which is called __predicate locks__\n",
    "\n",
    "If you read some set of records in the beginning of transaction,\n",
    "and before your commit some other transaction did something which changes the result of the same query if it would be repeated now - your transaction will be rejected.\n",
    "It makes a lot of sense - your logic could depend on previous read results, and your actions would be different now, when data was changed.\n",
    "\n",
    "Let's look how it solves previous example of checking for table-wide uniqueness:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_table()\n",
    "\n",
    "unique_name = 'Unique'\n",
    "\n",
    "\n",
    "# Both transactions should be serial. !!\n",
    "with (\n",
    "    SerializableSession() as Alice,\n",
    "    SerializableSession() as Bob\n",
    "):\n",
    "    #1 Alice checks the name is not taken, and takes it\n",
    "    found = Alice.query(Product).filter(Product.name == unique_name).all()\n",
    "\n",
    "    assert not found, \"Alice decides the name is not taken\"\n",
    "\n",
    "    Alice.add(Product(name=unique_name))\n",
    "\n",
    "    #2 Bob does the same\n",
    "    found = Bob.query(Product).filter(Product.name == unique_name).all()\n",
    "\n",
    "    assert not found, \"Bob decides the name is not taken\"\n",
    "\n",
    "    Bob.add(Product(name=unique_name))\n",
    "\n",
    "    #3\n",
    "    Alice.commit()\n",
    "\n",
    "    #4\n",
    "    with pytest.raises(Exception) as excinfo:\n",
    "        Bob.commit()\n",
    "    assert (\n",
    "        \"could not serialize access due to read/write dependencies among transactions\" in str(excinfo.value)\n",
    "    )\n",
    "\n",
    "found = ses_autocommit.query(Product).filter(Product.name == unique_name).all()\n",
    "assert len(found) == 1, \"Uniqness checking logic was not bypassed! :)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**How technically this is implemented?**\n",
    "\n",
    "So called __predicate locks__ are used. Conceptually, on each your `SELECT` query, the predicate is saved (basically, `WHERE` condition, defining the set of rows returned). They doesn't really lock anything, but just are used to find out dependencies between transactions. If it's not possible to reorder and serialize them (which means, cycles present), a transaction (the last one) is rejected. \n",
    "\n",
    "This is a difference from optimistic row-level locks, available at RR,\n",
    "where last transaction is always rejected.\n",
    "In the case predicate logs, Postgres will try to reorder transactions, and only if it's not possible - reject one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, you see a reordering of transactions. \n",
    "Alice already created a product and commited by the time\n",
    "Bob takes a decision based on already outdated information.\n",
    "Despite that, it's possible to reorder transactions (first Bob, then Alice) making both Alice'c and Bob's actions consistent.[^diff_from_prev]\n",
    "This is quite fair: which transaction would be committed first, Alice or Bob, is anyway a random choice.\n",
    "\n",
    "[^diff_from_prev]: The subtle difference from the previous example is only in the step 2. When Bob creates record in `products` table, it makes transactions non-serializable. When there's a write to `users` table - all good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_table()\n",
    "\n",
    "unique_name = 'Unique'\n",
    "\n",
    "\n",
    "# Both transactions should be serial. !!\n",
    "with (\n",
    "    SerializableSession() as Alice,\n",
    "    SerializableSession() as Bob\n",
    "):\n",
    "    #1 Alice checks the name is not taken, and takes it\n",
    "    found = Alice.query(Product).filter(Product.name == unique_name).all()\n",
    "\n",
    "    assert not found, \"Alice decides the name is not taken\"\n",
    "\n",
    "    Alice.add(Product(name=unique_name))\n",
    "\n",
    "    #2 Bob does the same\n",
    "    found = Bob.query(Product).filter(Product.name == unique_name).all()\n",
    "\n",
    "    assert not found, \"Bob decides the name is not taken\"\n",
    "\n",
    "    Bob.add(\n",
    "        User(name=f\"I'm Bob who observed {len(found)} products in db.\")\n",
    "    )\n",
    "    \n",
    "    #3\n",
    "    Alice.commit()\n",
    "\n",
    "    #4\n",
    "\n",
    "    Bob.commit()\n",
    "\n",
    "\n",
    "found = ses_autocommit.query(Product).filter(Product.name == unique_name).all()\n",
    "assert len(found) == 1, \"Uniqness checking logic was not bypassed! :)\"\n",
    "\n",
    "assert (\n",
    "    ses_autocommit.query(User).first().name \n",
    "    == \"I'm Bob who observed 0 products in db.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to clarify: **both transactions should be of Serializable level** to enforce this behaviour. In other words - those checks works only between serializable transactions. If some transaction on lower level changes the data that you read inside Serializable, nothing will prevent you to commit (and probably make an error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "## Case of one transaction of lower level\n",
    "\n",
    "clean_table()\n",
    "\n",
    "# Both transactions should be serial. !!\n",
    "with (\n",
    "    SerializableSession() as Alice,\n",
    "    ReadCommittedSession() as Bob  \n",
    "):\n",
    "    #1\n",
    "    read_products = list(Alice.query(Product).all() )\n",
    "    print(read_products)\n",
    "\n",
    "    # 2\n",
    "    Bob.add(Product(name='this product is something new, not seen by Alice yet'))\n",
    "    Bob.commit()\n",
    "    # 3\n",
    "    \n",
    "    Alice.add(Product(name='something'))\n",
    "    Alice.commit()\n",
    "\n",
    "assert ses_autocommit.query(Product).count() == 2, \"both transactions accepted\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary - mental model\n",
    "\n",
    "Let's summarize isolation levels of Postgres.\n",
    "This is a table taken from official postgres documention:\n",
    "![Isolation levels (from postgres doc.)](Postgres_iso_levels.png){#fig-postgres-levels}\n",
    "\n",
    "I propose an alternative model of thinking about these levels, \n",
    "which I find more simple and easier to remember.\n",
    "\n",
    "| level | Which data you can see/modify?[^my-data]  | Optimistic locking? | What happens on commit? |\n",
    "|---------|:-----|------:|:------:|\n",
    "| RC      | Fresh current state, committed so far  |    No |   _   |\n",
    "| RR     | Snapshot - comitted by the start of tr.  |   On records you modified |  Abort, if conflict detected   |\n",
    "| S       | Snapshot - comitted by the start of tr.    |  Plus, on predicates you used (either in SELECT or bulk UPDATE)  |   Try to reorder transactions, if not possible - abort the last one     |\n",
    "\n",
    ": My model of isolation levels {#tbl-letters}\n",
    "\n",
    "[^my-data]: Besides created by me during this transaction execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
